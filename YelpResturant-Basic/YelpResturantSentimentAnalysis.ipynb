{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas to be able to work with CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# Import Counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews=pd.read_csv(\"yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>VY_tvNUCCXGXQeSvJl757Q</td>\n",
       "      <td>2012-07-28</td>\n",
       "      <td>Ubyfp2RSDYW0g7Mbr8N3iA</td>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>review</td>\n",
       "      <td>_eqQoPtQ3e3UxLE4faT6ow</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>EKzMHI1tip8rC1-ZAy64yg</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>2XyIOQKbVFb6uXQdJ0RzlQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n",
       "      <td>review</td>\n",
       "      <td>ROru4uk5SaYc3rg8IU7SQw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>jyznYkIbpqVmlsZxSDSypA</td>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>review</td>\n",
       "      <td>gGbN1aKQHMgfQZkqlsuwzg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9SKdOoDHcFoxK5ZtsgHJoA</td>\n",
       "      <td>2012-12-02</td>\n",
       "      <td>5UKq9WQE1qQbJ0DJbc-B6Q</td>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>review</td>\n",
       "      <td>0lyVoNazXa20WzUyZPLaQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pF7uRzygyZsltbmVpjIyvw</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>vWSmOhg2ID1MNZHaWapGbA</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>review</td>\n",
       "      <td>KSBFytcdjPKZgXKQnYQdkA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "0     9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1     ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2     6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3     _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4     6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "...                      ...         ...                     ...    ...   \n",
       "9995  VY_tvNUCCXGXQeSvJl757Q  2012-07-28  Ubyfp2RSDYW0g7Mbr8N3iA      3   \n",
       "9996  EKzMHI1tip8rC1-ZAy64yg  2012-01-18  2XyIOQKbVFb6uXQdJ0RzlQ      4   \n",
       "9997  53YGfwmbW73JhFiemNeyzQ  2010-11-16  jyznYkIbpqVmlsZxSDSypA      4   \n",
       "9998  9SKdOoDHcFoxK5ZtsgHJoA  2012-12-02  5UKq9WQE1qQbJ0DJbc-B6Q      2   \n",
       "9999  pF7uRzygyZsltbmVpjIyvw  2010-10-16  vWSmOhg2ID1MNZHaWapGbA      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "0     My wife took me here on my birthday for breakf...  review   \n",
       "1     I have no idea why some people give bad review...  review   \n",
       "2     love the gyro plate. Rice is so good and I als...  review   \n",
       "3     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4     General Manager Scott Petello is a good egg!!!...  review   \n",
       "...                                                 ...     ...   \n",
       "9995  First visit...Had lunch here today - used my G...  review   \n",
       "9996  Should be called house of deliciousness!\\n\\nI ...  review   \n",
       "9997  I recently visited Olive and Ivy for business ...  review   \n",
       "9998  My nephew just moved to Scottsdale recently so...  review   \n",
       "9999  4-5 locations.. all 4.5 star average.. I think...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "0     rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1     0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2     0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3     uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4     vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
       "...                      ...   ...     ...    ...  \n",
       "9995  _eqQoPtQ3e3UxLE4faT6ow     1       2      0  \n",
       "9996  ROru4uk5SaYc3rg8IU7SQw     0       0      0  \n",
       "9997  gGbN1aKQHMgfQZkqlsuwzg     0       0      0  \n",
       "9998  0lyVoNazXa20WzUyZPLaQQ     0       0      0  \n",
       "9999  KSBFytcdjPKZgXKQnYQdkA     0       0      0  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintReviewOfInterest(index):\n",
    "    print(Reviews.iloc[index].text)\n",
    "    print(\"The Number of Stars is \"+str(Reviews.iloc[index].stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n",
      "The Number of Stars is 5\n"
     ]
    }
   ],
   "source": [
    "PrintReviewOfInterest(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Helaly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords   # to remove all the stop words\n",
    "from nltk.tokenize import word_tokenize  # Convert words to tokens\n",
    "from nltk import download as DownloadFromNltk\n",
    "\n",
    "DownloadFromNltk('stopwords')\n",
    "# Get all the stop words\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove Puncutation marks.\n",
    "def CleanTextData(text):\n",
    "    # Move all to lower case\n",
    "    text = text.lower()\n",
    "    # Remove all Weird stuff\n",
    "    text = text.replace(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = text.rplace(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "                 \n",
    "# Function to remove Stop Words that doesn't add any value\n",
    "def RemoveStopWords(text):\n",
    "    tokens = word_tokenize(text)  \n",
    "    filtered_tokens = [w for w in word_tokens if not w in stop_words]  \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviewText(Review):\n",
    "    return Review.text\n",
    "\n",
    "def GetReviewRating(Review):\n",
    "    StarRating= Review.stars\n",
    "    if StarRating>=3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "6000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# PreProcess the data\n",
    "\n",
    "# Shuffle the dataset , so if the data is sorted by stars we are now sure that it's not.\n",
    "Reviews = Reviews.sample(frac = 1) \n",
    "\n",
    "RatioOfTraining= 0.6\n",
    "RationOfTesting= 0.2\n",
    "RatioOfValidation= 0.2\n",
    "\n",
    "items={}\n",
    "\n",
    "TotalLengthOfData= len(Reviews)\n",
    "LengthOfTrain= int(TotalLengthOfData * RatioOfTraining)\n",
    "LengthOfValidation= int(TotalLengthOfData * RatioOfValidation)\n",
    "LengthOfTesting=  int(TotalLengthOfData * RationOfTesting)\n",
    "\n",
    "print(TotalLengthOfData)\n",
    "print(LengthOfTrain)\n",
    "print(LengthOfValidation)\n",
    "print(LengthOfTesting)\n",
    "\n",
    "X_Train=[];Y_Train=[]\n",
    "X_Val=[];Y_Val=[]\n",
    "X_Test=[];Y_Test=[]\n",
    "\n",
    "for i in range(LengthOfTrain):\n",
    "    CurrentReview= Reviews.iloc[i]\n",
    "    X_Train.append(GetReviewText(CurrentReview))\n",
    "    Y_Train.append(GetReviewRating(CurrentReview))\n",
    "    \n",
    "for i in range(LengthOfTrain,LengthOfTrain+LengthOfValidation):\n",
    "    CurrentReview= Reviews.iloc[i]\n",
    "    X_Val.append(GetReviewText(CurrentReview))\n",
    "    Y_Val.append(GetReviewRating(CurrentReview))\n",
    "    \n",
    "for i in range(LengthOfValidation,LengthOfValidation+LengthOfTesting):\n",
    "    CurrentReview= Reviews.iloc[i]\n",
    "    X_Test.append(GetReviewText(CurrentReview))\n",
    "    Y_Test.append(GetReviewRating(CurrentReview))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class YelpDataSet(Dataset):\n",
    "    def __init__(self,X_Train,Y_Train,X_Val,Y_Val,X_Test,Y_Test,vectorizer):\n",
    "        \n",
    "            self.X_Train= X_Train\n",
    "            self.Y_Train= Y_Train\n",
    "            self.X_Val= X_Val\n",
    "            self.Y_Val= Y_Val\n",
    "            self.X_Test= X_Test\n",
    "            self.Y_Test= Y_Test\n",
    "            self._lookup_dict = {   'train': (self.X_Train,self.Y_Train,len(self.X_Train)),\n",
    "                                    'val': (self.X_Val,self.Y_Val,len(self.X_Val)),\n",
    "                                    'test': (self.X_Test,self.Y_Test,len(self.X_Test))\n",
    "                                }\n",
    "            self._vectorizer= vectorizer\n",
    "            \n",
    "    def set_split(self, split=\"train\"):\n",
    "            self._target_split = split\n",
    "            self._target_x,self._target_y, self._target_size = self._lookup_dict[split]\n",
    "    def __len__(self):\n",
    "            return self._target_size\n",
    "    def __getitem__(self, index):\n",
    "            row = self._target_x[index]\n",
    "            review_vector= self._vectorizer.vectorize(row)\n",
    "            return {'x_data': review_vector,\n",
    "            'y_target': self._target_y[index]}\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "            return len(self) // batch_size\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,X_Train,Y_Train,X_Val,Y_Val,X_Test,Y_Test):\n",
    "        return cls(X_Train,Y_Train,X_Val,Y_Val,X_Test,Y_Test, ReviewVectorizer.from_dataset(X_Train,X_Val,X_Test))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "        def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "            if token_to_idx is None:\n",
    "                token_to_idx = {}\n",
    "            self._token_to_idx = token_to_idx\n",
    "            self._idx_to_token = {  idx: token\n",
    "                                    for token, idx in self._token_to_idx.items() }\n",
    "            self.add_unk = add_unk\n",
    "            self._unk_token = unk_token\n",
    "            self.unk_index = -1\n",
    "            if add_unk:\n",
    "                self.unk_index = self.add_token(unk_token)\n",
    "        def to_serializable(self):\n",
    "            return {'token_to_idx': self._token_to_idx,\n",
    "                    'add_unk': self._add_unk,\n",
    "                    'unk_token': self._unk_token}\n",
    "        @classmethod\n",
    "        def from_serializable(cls, contents):\n",
    "            return cls(**contents)\n",
    "\n",
    "        def add_token(self, token):\n",
    "            if token in self._token_to_idx:\n",
    "                index = self._token_to_idx[token]\n",
    "            else:\n",
    "                index = len(self._token_to_idx)\n",
    "                self._token_to_idx[token] = index\n",
    "                self._idx_to_token[index] = token\n",
    "            return index\n",
    "\n",
    "        def lookup_token(self, token):\n",
    "            if self.add_unk:\n",
    "                return self._token_to_idx.get(token, self.unk_index)\n",
    "            else:\n",
    "                return self._token_to_idx[token]\n",
    "\n",
    "        def lookup_index(self, index):\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "        def __str__(self):\n",
    "            return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "        def __len__(self):\n",
    "            return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class   ReviewVectorizer(object):   \n",
    "        def __init__(self, review_vocab):\n",
    "            self.review_vocab = review_vocab\n",
    "        \n",
    "        def vectorize(self, review):\n",
    "            one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "            for token in review.split(\" \"):\n",
    "                if  token not in string.punctuation:\n",
    "                    one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "            return one_hot\n",
    "        \n",
    "        @classmethod\n",
    "        def from_dataset(cls, X_Train,X_Val,X_Test, cutoff=25):\n",
    "            Reviews= []\n",
    "            Reviews.extend(X_Train)\n",
    "            Reviews.extend(X_Val)\n",
    "            Reviews.extend(X_Test)\n",
    "            review_vocab = Vocabulary(add_unk=True)\n",
    "            word_counts = Counter()\n",
    "            for review in Reviews:\n",
    "                for word in review.split(\" \"):\n",
    "                    if  word not in string.punctuation:\n",
    "                        word_counts[word] += 1\n",
    "            for word, count in word_counts.items():\n",
    "                if  count > cutoff:\n",
    "                    review_vocab.add_token(word)\n",
    "            return cls(review_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def generate_batches(dataset, batch_size, shuffle=True,drop_last=True, device=\"cuda\"):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "    shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        #print(data_dict)\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "    yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ReviewClassifier(nn.Module):  \n",
    "    def __init__(self, num_features):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features,\n",
    "                            out_features=1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Routine\n",
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "                    frequency_cutoff=25,\n",
    "                    model_state_file='model.pth',\n",
    "                    save_dir='model_storage/ch3/yelp/',\n",
    "                    early_stopping_criteria=5,\n",
    "                    learning_rate=0.001,\n",
    "                    num_epochs=20,\n",
    "                    seed=1337,\n",
    "                    batch_size=128,\n",
    "                    device=\"cuda\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_train_state():\n",
    "    return {'epoch_index': 0,\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'test_loss': -1,\n",
    "    'test_acc': -1}\n",
    "\n",
    "train_state = make_train_state()\n",
    "# dataset and vectorizer\n",
    "dataset = YelpDataSet.load_dataset_and_make_vectorizer(X_Train,Y_Train,X_Val,Y_Val,X_Test,Y_Test)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "# model\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to('cuda')\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First time at YC\\'s, had been to other Mongolian BBQ style places, none as \"upscale\" as this. Most of the Mongo places I\\'ve been are in the mall fast food court, or in a small strip mall. This place made us go WOW as to how big it was and all the food you get for about $11 a person for dinner.\\n\\nTo go boxes are 50 cents, super reasonable considering you can really cram a lot of food into your bowl to get cooked. I was actually shocked that they suggested you pack the food into your bowl to get more space! Staff was friendly and even helped us out with mixing the sauces.\\n\\nThis place is super close to our house and we will definitely be back frequently.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X_Train[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.Y_Train[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_accuracy(Target,Prediciton):\n",
    "    Target=Target.type(torch.LongTensor)\n",
    "    Prediciton=Prediciton.type(torch.LongTensor)\n",
    "    NumberOfEqualElements=torch.sum(Target==Prediciton)\n",
    "    NumberOfEqualElements=NumberOfEqualElements.item()\n",
    "    return NumberOfEqualElements/len(Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5012668371200562\n",
      "0.4978640675544739\n",
      "0.5349793434143066\n",
      "0.5608023405075073\n",
      "0.44612109661102295\n",
      "0.45190832018852234\n",
      "0.4404187500476837\n",
      "0.4516959488391876\n",
      "0.5483689308166504\n",
      "0.4948042035102844\n",
      "0.4916734993457794\n",
      "0.4444654583930969\n",
      "0.537704586982727\n",
      "0.44415026903152466\n",
      "0.4795745611190796\n",
      "0.47879788279533386\n",
      "0.48939281702041626\n",
      "0.48288559913635254\n",
      "0.4747862219810486\n",
      "0.39048492908477783\n",
      "0.44412967562675476\n",
      "0.4405059218406677\n",
      "0.5041122436523438\n",
      "0.48468711972236633\n",
      "0.48028576374053955\n",
      "0.3752189576625824\n",
      "0.4491419494152069\n",
      "0.42838799953460693\n",
      "0.4355907738208771\n",
      "0.48833414912223816\n",
      "0.4491213262081146\n",
      "0.4713687598705292\n",
      "0.455226868391037\n",
      "0.5131123065948486\n",
      "0.47145602107048035\n",
      "0.42369064688682556\n",
      "0.42158740758895874\n",
      "0.4535064697265625\n",
      "0.41745591163635254\n",
      "0.3984973430633545\n",
      "0.48118332028388977\n",
      "0.3899977207183838\n",
      "0.4348806142807007\n",
      "0.4684370756149292\n",
      "0.42799729108810425\n",
      "0.4191325306892395\n",
      "0.4523411989212036\n",
      "0.4503563642501831\n",
      "0.47844547033309937\n",
      "0.3609922528266907\n",
      "0.47684067487716675\n",
      "0.3855828642845154\n",
      "0.4193497896194458\n",
      "0.4569721817970276\n",
      "0.434702068567276\n",
      "0.4531717896461487\n",
      "0.37672361731529236\n",
      "0.39954209327697754\n",
      "0.435991108417511\n",
      "0.41879138350486755\n",
      "0.4181511700153351\n",
      "0.43151769042015076\n",
      "0.3990820050239563\n",
      "0.3890889286994934\n",
      "0.4100377857685089\n",
      "0.4025939106941223\n",
      "0.4319976568222046\n",
      "0.38170188665390015\n",
      "0.3826093375682831\n",
      "0.4257779121398926\n",
      "0.3905879259109497\n",
      "0.3999382257461548\n",
      "0.3875759243965149\n",
      "0.3983825743198395\n",
      "0.40674060583114624\n",
      "0.454461932182312\n",
      "0.3878876566886902\n",
      "0.4051493704319\n",
      "0.36854633688926697\n",
      "0.4180613160133362\n",
      "0.3290393054485321\n",
      "0.3882162272930145\n",
      "0.39948078989982605\n",
      "0.388547420501709\n",
      "0.3759494423866272\n",
      "0.37579429149627686\n",
      "0.404960036277771\n",
      "0.35821619629859924\n",
      "0.44745418429374695\n",
      "0.3836671710014343\n",
      "0.4060697555541992\n",
      "0.41189831495285034\n",
      "0.38819244503974915\n",
      "0.3098472058773041\n",
      "0.35087335109710693\n",
      "0.4305208921432495\n",
      "0.34277188777923584\n",
      "0.37951353192329407\n",
      "0.3783470094203949\n",
      "0.33859607577323914\n",
      "0.3831123113632202\n",
      "0.4039022922515869\n",
      "0.3817773759365082\n",
      "0.3769107758998871\n",
      "0.36393997073173523\n",
      "0.41383588314056396\n",
      "0.3938804268836975\n",
      "0.38557231426239014\n",
      "0.4336683750152588\n",
      "0.3788790702819824\n",
      "0.40284743905067444\n",
      "0.3034506142139435\n",
      "0.3378802537918091\n",
      "0.37403565645217896\n",
      "0.4226837158203125\n",
      "0.3570564389228821\n",
      "0.3434463441371918\n",
      "0.3494958281517029\n",
      "0.3051230013370514\n",
      "0.3934270143508911\n",
      "0.3903389573097229\n",
      "0.3619611859321594\n",
      "0.3647453188896179\n",
      "0.3578065037727356\n",
      "0.40890562534332275\n",
      "0.4064846634864807\n",
      "0.3137078881263733\n",
      "0.33932530879974365\n",
      "0.36540618538856506\n",
      "0.3634812533855438\n",
      "0.37738561630249023\n",
      "0.36185771226882935\n",
      "0.3261566162109375\n",
      "0.3719596862792969\n",
      "0.35200902819633484\n",
      "0.3374808728694916\n",
      "0.35680752992630005\n",
      "0.3762649893760681\n",
      "0.3737480938434601\n",
      "0.3219115138053894\n",
      "0.3350656032562256\n",
      "0.30069825053215027\n",
      "0.3442765474319458\n",
      "0.394001305103302\n",
      "0.2927144765853882\n",
      "0.3620184361934662\n",
      "0.3800082802772522\n",
      "0.3354688286781311\n",
      "0.38867953419685364\n",
      "0.3448970913887024\n",
      "0.3260224461555481\n",
      "0.3978895843029022\n",
      "0.33975276350975037\n",
      "0.3057583272457123\n",
      "0.35326576232910156\n",
      "0.3480173349380493\n",
      "0.3774009346961975\n",
      "0.2935182452201843\n",
      "0.3152376413345337\n",
      "0.3345836400985718\n",
      "0.345441997051239\n",
      "0.37727057933807373\n",
      "0.3527228832244873\n",
      "0.3869202733039856\n",
      "0.32618704438209534\n",
      "0.35300227999687195\n",
      "0.3595896363258362\n",
      "0.35131847858428955\n",
      "0.3309977054595947\n",
      "0.32895562052726746\n",
      "0.28461453318595886\n",
      "0.35845547914505005\n",
      "0.303435355424881\n",
      "0.32134300470352173\n",
      "0.36388981342315674\n",
      "0.3692786693572998\n",
      "0.34360337257385254\n",
      "0.3386476933956146\n",
      "0.34788405895233154\n",
      "0.32757553458213806\n",
      "0.30700966715812683\n",
      "0.3401469886302948\n",
      "0.2774417996406555\n",
      "0.38497644662857056\n",
      "0.2998562455177307\n",
      "0.31673291325569153\n",
      "0.2943476140499115\n",
      "0.28771471977233887\n",
      "0.37149155139923096\n",
      "0.33080437779426575\n",
      "0.34038907289505005\n",
      "0.30765968561172485\n",
      "0.3319096863269806\n",
      "0.24819543957710266\n",
      "0.35296356678009033\n",
      "0.2923715114593506\n",
      "0.3399777114391327\n",
      "0.24931126832962036\n",
      "0.38446807861328125\n",
      "0.3420180082321167\n",
      "0.33088213205337524\n",
      "0.33810073137283325\n",
      "0.3166368901729584\n",
      "0.319317489862442\n",
      "0.34312814474105835\n",
      "0.3686671853065491\n",
      "0.3217373788356781\n",
      "0.3819076716899872\n",
      "0.2779926061630249\n",
      "0.30466195940971375\n",
      "0.29660701751708984\n",
      "0.30066537857055664\n",
      "0.31489017605781555\n",
      "0.27695971727371216\n",
      "0.37984156608581543\n",
      "0.4266958236694336\n",
      "0.3648829460144043\n",
      "0.3047122061252594\n",
      "0.2871363162994385\n",
      "0.3044488728046417\n",
      "0.27996811270713806\n",
      "0.2978065013885498\n",
      "0.25943291187286377\n",
      "0.32500767707824707\n",
      "0.321442186832428\n",
      "0.23083478212356567\n",
      "0.34039101004600525\n",
      "0.2542169690132141\n",
      "0.3435601592063904\n",
      "0.26914656162261963\n",
      "0.30413925647735596\n",
      "0.31671342253685\n",
      "0.3503514230251312\n",
      "0.34448856115341187\n",
      "0.2913002371788025\n",
      "0.3330801725387573\n",
      "0.2907273769378662\n",
      "0.2758282423019409\n",
      "0.34953516721725464\n",
      "0.2769944667816162\n",
      "0.3131345510482788\n",
      "0.3048095107078552\n",
      "0.36256760358810425\n",
      "0.3155517578125\n",
      "0.31863605976104736\n",
      "0.36509576439857483\n",
      "0.32275381684303284\n",
      "0.31763795018196106\n",
      "0.3411640226840973\n",
      "0.29167938232421875\n",
      "0.34038662910461426\n",
      "0.24427644908428192\n",
      "0.34486669301986694\n",
      "0.3177565634250641\n",
      "0.28681042790412903\n",
      "0.3338901996612549\n",
      "0.32281070947647095\n",
      "0.3500347137451172\n",
      "0.2796616554260254\n",
      "0.28972259163856506\n",
      "0.2805878520011902\n",
      "0.3283742666244507\n",
      "0.2788117527961731\n",
      "0.29782938957214355\n",
      "0.2345694899559021\n",
      "0.33480966091156006\n",
      "0.24566784501075745\n",
      "0.3143513798713684\n",
      "0.24857816100120544\n",
      "0.2737918794155121\n",
      "0.23738062381744385\n",
      "0.2845619320869446\n",
      "0.28056323528289795\n",
      "0.3031077980995178\n",
      "0.35765910148620605\n",
      "0.2901354432106018\n",
      "0.2926672101020813\n",
      "0.278359979391098\n",
      "0.34865090250968933\n",
      "0.2982727885246277\n",
      "0.2697049677371979\n",
      "0.28945982456207275\n",
      "0.2595023512840271\n",
      "0.2941925823688507\n",
      "0.26514875888824463\n",
      "0.3022802472114563\n",
      "0.36176830530166626\n",
      "0.30094826221466064\n",
      "0.2897709906101227\n",
      "0.2840083837509155\n",
      "0.316753625869751\n",
      "0.293222039937973\n",
      "0.27683883905410767\n",
      "0.2468601018190384\n",
      "0.29642146825790405\n",
      "0.31506362557411194\n",
      "0.3174239695072174\n",
      "0.2753305733203888\n",
      "0.30330777168273926\n",
      "0.2586803436279297\n",
      "0.27043288946151733\n",
      "0.33216261863708496\n",
      "0.25603440403938293\n",
      "0.26791903376579285\n",
      "0.2925774157047272\n",
      "0.32869264483451843\n",
      "0.27601560950279236\n",
      "0.2978939712047577\n",
      "0.2207193672657013\n",
      "0.321601927280426\n",
      "0.35039910674095154\n",
      "0.24752186238765717\n",
      "0.26317816972732544\n",
      "0.26551178097724915\n",
      "0.2861960232257843\n",
      "0.3480439782142639\n",
      "0.30512937903404236\n",
      "0.27728334069252014\n",
      "0.30353283882141113\n",
      "0.30335161089897156\n",
      "0.26630908250808716\n",
      "0.25710272789001465\n",
      "0.29715755581855774\n",
      "0.30294740200042725\n",
      "0.37497302889823914\n",
      "0.2818964123725891\n",
      "0.29295456409454346\n",
      "0.33857131004333496\n",
      "0.27639228105545044\n",
      "0.2692667245864868\n",
      "0.34186410903930664\n",
      "0.3110893964767456\n",
      "0.31202417612075806\n",
      "0.3124716281890869\n",
      "0.31800854206085205\n",
      "0.31687164306640625\n",
      "0.2728272080421448\n",
      "0.30217722058296204\n",
      "0.2440042793750763\n",
      "0.2934432029724121\n",
      "0.31025105714797974\n",
      "0.3472203016281128\n",
      "0.3359846770763397\n",
      "0.2945336401462555\n",
      "0.2831455171108246\n",
      "0.30022096633911133\n",
      "0.2396976351737976\n",
      "0.2716229259967804\n",
      "0.288750559091568\n",
      "0.296378493309021\n",
      "0.26967155933380127\n",
      "0.2887512445449829\n",
      "0.26444917917251587\n",
      "0.252032995223999\n",
      "0.27791374921798706\n",
      "0.2643253207206726\n",
      "0.24894264340400696\n",
      "0.30837810039520264\n",
      "0.26073595881462097\n",
      "0.24644632637500763\n",
      "0.22510914504528046\n",
      "0.29605531692504883\n",
      "0.2444998323917389\n",
      "0.2592821717262268\n",
      "0.2267935574054718\n",
      "0.2600330710411072\n",
      "0.2760562300682068\n",
      "0.27454429864883423\n",
      "0.2748406231403351\n",
      "0.3042270243167877\n",
      "0.21694454550743103\n",
      "0.2943119704723358\n",
      "0.25674986839294434\n",
      "0.3240410089492798\n",
      "0.24930661916732788\n",
      "0.19277311861515045\n",
      "0.29893773794174194\n",
      "0.29143452644348145\n",
      "0.2715873420238495\n",
      "0.2645391821861267\n",
      "0.2890292704105377\n",
      "0.21766896545886993\n",
      "0.23566842079162598\n",
      "0.2477901577949524\n",
      "0.2614324688911438\n",
      "0.26839423179626465\n",
      "0.2721495032310486\n",
      "0.18987515568733215\n",
      "0.3244017958641052\n",
      "0.283759742975235\n",
      "0.2387736588716507\n",
      "0.2461968958377838\n",
      "0.266292929649353\n",
      "0.2752423882484436\n",
      "0.2330925464630127\n",
      "0.4161486029624939\n",
      "0.28339627385139465\n",
      "0.20788373053073883\n",
      "0.2698960304260254\n",
      "0.2890413999557495\n",
      "0.2242213636636734\n",
      "0.2816047966480255\n",
      "0.22445695102214813\n",
      "0.28768762946128845\n",
      "0.2960811257362366\n",
      "0.2387121319770813\n",
      "0.26045340299606323\n",
      "0.27019619941711426\n",
      "0.29487812519073486\n",
      "0.2477661669254303\n",
      "0.26412850618362427\n",
      "0.25541555881500244\n",
      "0.3245682120323181\n",
      "0.28741586208343506\n",
      "0.21076862514019012\n",
      "0.24680304527282715\n",
      "0.27450791001319885\n",
      "0.25976458191871643\n",
      "0.22937637567520142\n",
      "0.23293377459049225\n",
      "0.2810730040073395\n",
      "0.2751721143722534\n",
      "0.23971134424209595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2086067944765091\n",
      "0.273884117603302\n",
      "0.26945334672927856\n",
      "0.24674582481384277\n",
      "0.28608471155166626\n",
      "0.2177382856607437\n",
      "0.25521230697631836\n",
      "0.23838260769844055\n",
      "0.2327878177165985\n",
      "0.25416645407676697\n",
      "0.2606227993965149\n",
      "0.20382967591285706\n",
      "0.24083617329597473\n",
      "0.22671306133270264\n",
      "0.2960025668144226\n",
      "0.1866483986377716\n",
      "0.263672411441803\n",
      "0.21535417437553406\n",
      "0.2061643749475479\n",
      "0.23088760673999786\n",
      "0.23925915360450745\n",
      "0.2434808760881424\n",
      "0.28643152117729187\n",
      "0.2097851186990738\n",
      "0.2627725899219513\n",
      "0.29990682005882263\n",
      "0.24065187573432922\n",
      "0.30327510833740234\n",
      "0.2352234572172165\n",
      "0.2716641426086426\n",
      "0.22527827322483063\n",
      "0.19745175540447235\n",
      "0.2813699245452881\n",
      "0.25803864002227783\n",
      "0.29941076040267944\n",
      "0.26349055767059326\n",
      "0.28076988458633423\n",
      "0.2006206512451172\n",
      "0.26840174198150635\n",
      "0.2003600001335144\n",
      "0.24825744330883026\n",
      "0.299410879611969\n",
      "0.21118125319480896\n",
      "0.24953490495681763\n",
      "0.23214936256408691\n",
      "0.25903284549713135\n",
      "0.2739714980125427\n",
      "0.2730812728404999\n",
      "0.2856515049934387\n",
      "0.18968378007411957\n",
      "0.19683007895946503\n",
      "0.3124787211418152\n",
      "0.2857089042663574\n",
      "0.2287401705980301\n",
      "0.26442772150039673\n",
      "0.25885093212127686\n",
      "0.23375658690929413\n",
      "0.2283203899860382\n",
      "0.2512160539627075\n",
      "0.22881031036376953\n",
      "0.25615864992141724\n",
      "0.2679973244667053\n",
      "0.21320894360542297\n",
      "0.2412487119436264\n",
      "0.31494608521461487\n",
      "0.24745263159275055\n",
      "0.23863562941551208\n",
      "0.2547113001346588\n",
      "0.23979118466377258\n",
      "0.2604866921901703\n",
      "0.2907341420650482\n",
      "0.231166809797287\n",
      "0.24323630332946777\n",
      "0.20837262272834778\n",
      "0.24802596867084503\n",
      "0.3258483111858368\n",
      "0.3420872390270233\n",
      "0.21673142910003662\n",
      "0.2333325743675232\n",
      "0.28795334696769714\n",
      "0.26942574977874756\n",
      "0.20174431800842285\n",
      "0.23746520280838013\n",
      "0.22548863291740417\n",
      "0.17640170454978943\n",
      "0.25040602684020996\n",
      "0.2453814446926117\n",
      "0.22834521532058716\n",
      "0.22161853313446045\n",
      "0.2917511463165283\n",
      "0.2807508111000061\n",
      "0.22597824037075043\n",
      "0.21267704665660858\n",
      "0.2674669921398163\n",
      "0.28298264741897583\n",
      "0.30599188804626465\n",
      "0.2079716920852661\n",
      "0.23770181834697723\n",
      "0.2032032161951065\n",
      "0.24828343093395233\n",
      "0.2986450493335724\n",
      "0.2503124475479126\n",
      "0.2756909132003784\n",
      "0.26233386993408203\n",
      "0.1953643560409546\n",
      "0.16940268874168396\n",
      "0.2232958972454071\n",
      "0.2518312633037567\n",
      "0.23013661801815033\n",
      "0.25280019640922546\n",
      "0.27290549874305725\n",
      "0.2867404818534851\n",
      "0.24253439903259277\n",
      "0.2151893824338913\n",
      "0.2686800956726074\n",
      "0.2660355567932129\n",
      "0.24553632736206055\n",
      "0.21766531467437744\n",
      "0.1978490948677063\n",
      "0.1811283528804779\n",
      "0.2557212710380554\n",
      "0.2448200285434723\n",
      "0.30525580048561096\n",
      "0.17456859350204468\n",
      "0.2120192050933838\n",
      "0.23037749528884888\n",
      "0.1999603807926178\n",
      "0.24581073224544525\n",
      "0.22183042764663696\n",
      "0.25072258710861206\n",
      "0.2693898677825928\n",
      "0.21873924136161804\n",
      "0.21119752526283264\n",
      "0.30045345425605774\n",
      "0.2524186074733734\n",
      "0.385786235332489\n",
      "0.25856831669807434\n",
      "0.29712265729904175\n",
      "0.2219073325395584\n",
      "0.2430051863193512\n",
      "0.18795254826545715\n",
      "0.26751312613487244\n",
      "0.22566145658493042\n",
      "0.20489926636219025\n",
      "0.2195221185684204\n",
      "0.27692461013793945\n",
      "0.21979600191116333\n",
      "0.21753135323524475\n",
      "0.2315916121006012\n",
      "0.24205124378204346\n",
      "0.23223178088665009\n",
      "0.2059902846813202\n",
      "0.2191142737865448\n",
      "0.21400097012519836\n",
      "0.2788437604904175\n",
      "0.23818470537662506\n",
      "0.21097633242607117\n",
      "0.29762423038482666\n",
      "0.237448588013649\n",
      "0.26259610056877136\n",
      "0.18750795722007751\n",
      "0.19415801763534546\n",
      "0.2464769184589386\n",
      "0.25469934940338135\n",
      "0.24411852657794952\n",
      "0.22340723872184753\n",
      "0.22559772431850433\n",
      "0.249404639005661\n",
      "0.2301519513130188\n",
      "0.24558955430984497\n",
      "0.1987137496471405\n",
      "0.2088746726512909\n",
      "0.23035654425621033\n",
      "0.20027920603752136\n",
      "0.23096123337745667\n",
      "0.22119693458080292\n",
      "0.21443316340446472\n",
      "0.22841255366802216\n",
      "0.22215604782104492\n",
      "0.23004963994026184\n",
      "0.24257153272628784\n",
      "0.2977553904056549\n",
      "0.22132351994514465\n",
      "0.2435397505760193\n",
      "0.2715061604976654\n",
      "0.19035783410072327\n",
      "0.22604936361312866\n",
      "0.22198542952537537\n",
      "0.2474268078804016\n",
      "0.183709517121315\n",
      "0.22265104949474335\n",
      "0.20835182070732117\n",
      "0.2066350281238556\n",
      "0.21731418371200562\n",
      "0.19556984305381775\n",
      "0.21341103315353394\n",
      "0.19219546020030975\n",
      "0.27736523747444153\n",
      "0.24624451994895935\n",
      "0.20274963974952698\n",
      "0.20480358600616455\n",
      "0.20966237783432007\n",
      "0.24559137225151062\n",
      "0.23872148990631104\n",
      "0.2723492681980133\n",
      "0.20855732262134552\n",
      "0.22660967707633972\n",
      "0.26475009322166443\n",
      "0.19327785074710846\n",
      "0.2633885443210602\n",
      "0.19404897093772888\n",
      "0.20544683933258057\n",
      "0.24447442591190338\n",
      "0.22329843044281006\n",
      "0.24460119009017944\n",
      "0.22965694963932037\n",
      "0.21097005903720856\n",
      "0.22726081311702728\n",
      "0.27526116371154785\n",
      "0.24085284769535065\n",
      "0.21199098229408264\n",
      "0.2464192509651184\n",
      "0.2703348994255066\n",
      "0.20743246376514435\n",
      "0.21423828601837158\n",
      "0.23536336421966553\n",
      "0.28017911314964294\n",
      "0.24622687697410583\n",
      "0.2093980610370636\n",
      "0.21876247227191925\n",
      "0.20621517300605774\n",
      "0.20060905814170837\n",
      "0.2111269235610962\n",
      "0.24016551673412323\n",
      "0.19943886995315552\n",
      "0.2538730800151825\n",
      "0.20795875787734985\n",
      "0.2269171178340912\n",
      "0.22756865620613098\n",
      "0.1922120749950409\n",
      "0.19970478117465973\n",
      "0.21489952504634857\n",
      "0.2562759220600128\n",
      "0.2280237078666687\n",
      "0.2637670040130615\n",
      "0.21301205456256866\n",
      "0.19690553843975067\n",
      "0.198959082365036\n",
      "0.18777281045913696\n",
      "0.19835667312145233\n",
      "0.22183740139007568\n",
      "0.22645451128482819\n",
      "0.20148946344852448\n",
      "0.20727010071277618\n",
      "0.2725660800933838\n",
      "0.21138308942317963\n",
      "0.24096986651420593\n",
      "0.2000918686389923\n",
      "0.22626037895679474\n",
      "0.22373512387275696\n",
      "0.19441235065460205\n",
      "0.23884418606758118\n",
      "0.19114598631858826\n",
      "0.23624153435230255\n",
      "0.19109702110290527\n",
      "0.1796373724937439\n",
      "0.26344266533851624\n",
      "0.2205352783203125\n",
      "0.21338215470314026\n",
      "0.1583079993724823\n",
      "0.1927356719970703\n",
      "0.20570799708366394\n",
      "0.2607331871986389\n",
      "0.26292282342910767\n",
      "0.17066708207130432\n",
      "0.21928712725639343\n",
      "0.20952951908111572\n",
      "0.1762555092573166\n",
      "0.15521776676177979\n",
      "0.17180228233337402\n",
      "0.2058701068162918\n",
      "0.2126331925392151\n",
      "0.1562516689300537\n",
      "0.22184789180755615\n",
      "0.2069966197013855\n",
      "0.21560940146446228\n",
      "0.25283727049827576\n",
      "0.19931146502494812\n",
      "0.21843686699867249\n",
      "0.2298007756471634\n",
      "0.15933793783187866\n",
      "0.24766398966312408\n",
      "0.22498081624507904\n",
      "0.20265060663223267\n",
      "0.18541336059570312\n",
      "0.19457757472991943\n",
      "0.242636039853096\n",
      "0.22653356194496155\n",
      "0.16890370845794678\n",
      "0.23013538122177124\n",
      "0.2290203720331192\n",
      "0.18045689165592194\n",
      "0.2050635814666748\n",
      "0.1753717064857483\n",
      "0.21066385507583618\n",
      "0.17610549926757812\n",
      "0.2060476541519165\n",
      "0.21384423971176147\n",
      "0.2160303294658661\n",
      "0.2833729684352875\n",
      "0.1979825347661972\n",
      "0.18637502193450928\n",
      "0.2101181447505951\n",
      "0.23600175976753235\n",
      "0.15838715434074402\n",
      "0.19428829848766327\n",
      "0.18648961186408997\n",
      "0.25313830375671387\n",
      "0.22643470764160156\n",
      "0.18820765614509583\n",
      "0.2065970003604889\n",
      "0.20385409891605377\n",
      "0.22679784893989563\n",
      "0.157027006149292\n",
      "0.16946792602539062\n",
      "0.14968430995941162\n",
      "0.23417425155639648\n",
      "0.17882317304611206\n",
      "0.20212677121162415\n",
      "0.21387097239494324\n",
      "0.21432003378868103\n",
      "0.1909395158290863\n",
      "0.23232577741146088\n",
      "0.19768854975700378\n",
      "0.20578160881996155\n",
      "0.19824233651161194\n",
      "0.20097456872463226\n",
      "0.2031116783618927\n",
      "0.2138017863035202\n",
      "0.20487742125988007\n",
      "0.1647036373615265\n",
      "0.21514031291007996\n",
      "0.19174981117248535\n",
      "0.18768641352653503\n",
      "0.22446639835834503\n",
      "0.2195877730846405\n",
      "0.23402805626392365\n",
      "0.1910000592470169\n",
      "0.23255512118339539\n",
      "0.18946608901023865\n",
      "0.23394784331321716\n",
      "0.22927476465702057\n",
      "0.20578068494796753\n",
      "0.1849277913570404\n",
      "0.21320591866970062\n",
      "0.1993420422077179\n",
      "0.25792551040649414\n",
      "0.17769458889961243\n",
      "0.23337000608444214\n",
      "0.23587365448474884\n",
      "0.23225119709968567\n",
      "0.22269274294376373\n",
      "0.22468939423561096\n",
      "0.20222307741641998\n",
      "0.23385103046894073\n",
      "0.19007879495620728\n",
      "0.2809070944786072\n",
      "0.18994571268558502\n",
      "0.20421603322029114\n",
      "0.22244057059288025\n",
      "0.2274344265460968\n",
      "0.19447077810764313\n",
      "0.2348645031452179\n",
      "0.2021590769290924\n",
      "0.22593086957931519\n",
      "0.17697957158088684\n",
      "0.22308294475078583\n",
      "0.23471197485923767\n",
      "0.21283507347106934\n",
      "0.20222502946853638\n",
      "0.21365827322006226\n",
      "0.1839805692434311\n",
      "0.2041173279285431\n",
      "0.22921454906463623\n",
      "0.19405022263526917\n",
      "0.2040703296661377\n",
      "0.1892106533050537\n",
      "0.20854243636131287\n",
      "0.20101171731948853\n",
      "0.13532547652721405\n",
      "0.17188018560409546\n",
      "0.17778876423835754\n",
      "0.20053870975971222\n",
      "0.1853022277355194\n",
      "0.15450219810009003\n",
      "0.22055324912071228\n",
      "0.26763060688972473\n",
      "0.16768363118171692\n",
      "0.1989370435476303\n",
      "0.2121371626853943\n",
      "0.22145181894302368\n",
      "0.15370482206344604\n",
      "0.22247982025146484\n",
      "0.17170876264572144\n",
      "0.2331128865480423\n",
      "0.16146250069141388\n",
      "0.18682581186294556\n",
      "0.19171872735023499\n",
      "0.1958078145980835\n",
      "0.1715129166841507\n",
      "0.1904885321855545\n",
      "0.1858372986316681\n",
      "0.2124817669391632\n",
      "0.19645185768604279\n",
      "0.22116653621196747\n",
      "0.20281696319580078\n",
      "0.16759483516216278\n",
      "0.21016305685043335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16588151454925537\n",
      "0.1729777753353119\n",
      "0.16718903183937073\n",
      "0.18458861112594604\n",
      "0.1828317940235138\n",
      "0.21233662962913513\n",
      "0.21960417926311493\n",
      "0.2413882613182068\n",
      "0.19507500529289246\n",
      "0.19424369931221008\n",
      "0.17946559190750122\n",
      "0.219830721616745\n",
      "0.1558937281370163\n",
      "0.18207819759845734\n",
      "0.271714985370636\n",
      "0.18897753953933716\n",
      "0.20626917481422424\n",
      "0.16754409670829773\n",
      "0.2346392571926117\n",
      "0.17416587471961975\n",
      "0.23320215940475464\n",
      "0.1436261236667633\n",
      "0.21182787418365479\n",
      "0.17291298508644104\n",
      "0.19088715314865112\n",
      "0.1545274704694748\n",
      "0.19995589554309845\n",
      "0.2148086428642273\n",
      "0.20743969082832336\n",
      "0.19739821553230286\n",
      "0.1606670618057251\n",
      "0.22250717878341675\n",
      "0.17951755225658417\n",
      "0.21065497398376465\n",
      "0.17234882712364197\n",
      "0.2051946073770523\n",
      "0.16790643334388733\n",
      "0.23510867357254028\n",
      "0.16667994856834412\n",
      "0.1797284036874771\n",
      "0.22270281612873077\n",
      "0.21478405594825745\n",
      "0.17572984099388123\n",
      "0.22099292278289795\n",
      "0.17791327834129333\n",
      "0.20807163417339325\n",
      "0.2019120752811432\n",
      "0.17567996680736542\n",
      "0.2267671823501587\n",
      "0.18326880037784576\n",
      "0.21185976266860962\n",
      "0.14875254034996033\n",
      "0.17110657691955566\n",
      "0.243697851896286\n",
      "0.19599774479866028\n",
      "0.210852712392807\n",
      "0.18640543520450592\n",
      "0.1714673936367035\n",
      "0.1747605800628662\n",
      "0.2049582302570343\n",
      "0.20810061693191528\n",
      "0.15902233123779297\n",
      "0.1907031387090683\n",
      "0.18389514088630676\n",
      "0.20514509081840515\n",
      "0.19157959520816803\n",
      "0.21242710947990417\n",
      "0.24457702040672302\n",
      "0.14701390266418457\n",
      "0.17587910592556\n",
      "0.20412427186965942\n",
      "0.19149330258369446\n",
      "0.16508938372135162\n",
      "0.18599751591682434\n",
      "0.1830507516860962\n",
      "0.20521888136863708\n",
      "0.1778765320777893\n",
      "0.20925340056419373\n",
      "0.20622578263282776\n",
      "0.15347349643707275\n",
      "0.20986290276050568\n",
      "0.22484531998634338\n",
      "0.19562995433807373\n",
      "0.18614555895328522\n",
      "0.18001751601696014\n",
      "0.167559415102005\n",
      "0.21321821212768555\n",
      "0.16140404343605042\n",
      "0.21463823318481445\n",
      "0.19267556071281433\n",
      "0.17729595303535461\n",
      "0.18051990866661072\n",
      "0.1973026692867279\n",
      "0.17050492763519287\n",
      "0.24574944376945496\n",
      "0.20793652534484863\n",
      "0.15513941645622253\n",
      "0.18521438539028168\n",
      "0.19159387052059174\n",
      "0.15296314656734467\n",
      "0.207240492105484\n",
      "0.16628752648830414\n",
      "0.16650503873825073\n",
      "0.20645496249198914\n",
      "0.2352174073457718\n",
      "0.19796499609947205\n",
      "0.1672174632549286\n",
      "0.19187963008880615\n",
      "0.19219672679901123\n",
      "0.18380388617515564\n",
      "0.19097191095352173\n",
      "0.1800752878189087\n",
      "0.16580069065093994\n",
      "0.17737506330013275\n",
      "0.15703876316547394\n",
      "0.2225014567375183\n",
      "0.15491726994514465\n",
      "0.17781959474086761\n",
      "0.1916116625070572\n",
      "0.19758924841880798\n",
      "0.21588043868541718\n",
      "0.22192883491516113\n",
      "0.17175546288490295\n",
      "0.1848265379667282\n",
      "0.16476088762283325\n",
      "0.17669269442558289\n",
      "0.2239970862865448\n",
      "0.22293920814990997\n",
      "0.18200984597206116\n",
      "0.1846114993095398\n",
      "0.23163311183452606\n",
      "0.19518040120601654\n",
      "0.19649574160575867\n",
      "0.16599860787391663\n",
      "0.19167207181453705\n",
      "0.17913644015789032\n",
      "0.13731126487255096\n",
      "0.1861916184425354\n",
      "0.1835765540599823\n",
      "0.1799657940864563\n",
      "0.14945495128631592\n",
      "0.19981776177883148\n",
      "0.1550450325012207\n",
      "0.1721906214952469\n",
      "0.21118175983428955\n",
      "0.1666984260082245\n",
      "0.14407935738563538\n",
      "0.14591068029403687\n",
      "0.19038446247577667\n",
      "0.16153772175312042\n",
      "0.18205104768276215\n",
      "0.19909152388572693\n",
      "0.15276607871055603\n",
      "0.207124263048172\n",
      "0.20142045617103577\n",
      "0.19026994705200195\n",
      "0.12373021245002747\n",
      "0.23458178341388702\n",
      "0.17544516921043396\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in range(1000):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches( dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "    \n",
    "    running_loss = 0.1\n",
    "    running_acc = 0.1\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        #print(batch_index)\n",
    "        #print(batch_dict)\n",
    "        # the training routine is 5 steps:\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch/running_loss) / (batch_index + 1)\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        print(loss_batch)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch/running_acc) / (batch_index + 1)\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    # Iterate over val dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device)\n",
    "    running_loss = 0.1\n",
    "    running_acc = 0.1\n",
    "    classifier.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch/running_loss) / (batch_index + 1)\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch/ running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "batch_size=128,\n",
    "device=\"cuda\")\n",
    "running_loss = 0.1\n",
    "running_acc = 0.1\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch/running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch/ running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.033\n",
      "Test Accuracy: 2.68\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's overpriced\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review, classifier, vectorizer,\n",
    "    decision_threshold=0.5):\n",
    "    \"\"\"Predict the rating of a review\n",
    "    Args:\n",
    "    review (str): the text of the review\n",
    "    classifier (ReviewClassifier): the trained model\n",
    "    vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "    decision_threshold (float): The numerical boundary which\n",
    "    separates the rating classes\n",
    "    \"\"\"\n",
    "    #review = CleanTextData(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review)).to(\"cuda\")\n",
    "    #print(vectorized_review)\n",
    "    #print(vectorized_review.view(1, -1))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    return index\n",
    "test_review = \"it's overpriced\"\n",
    "#test_review= X_Train[2]\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print(test_review)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "Great\n",
      "Love\n",
      "great\n",
      ":)\n",
      "beat\n",
      "excellent\n",
      "helpful\n",
      "Excellent\n",
      "amazing!\n",
      "Plus,\n",
      "best\n",
      "excellent.\n",
      "fantastic,\n",
      "Thank\n",
      "awesome!\n",
      "favorite\n",
      "Friendly\n",
      "Super\n",
      "loves\n",
      "pleasantly\n"
     ]
    }
   ],
   "source": [
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.cpu().numpy().tolist()\n",
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "poor\n",
      "tasted\n",
      "customers\n",
      "overpriced\n",
      "bland.\n",
      "worst\n",
      "money\n",
      "disappointment.\n",
      "elsewhere.\n",
      "awful.\n",
      "Maybe\n",
      "asked\n",
      "not\n",
      "bland\n",
      "anymore.\n",
      "rude\n",
      "NO\n",
      "hamburger\n",
      "why\n",
      "then,\n"
     ]
    }
   ],
   "source": [
    "# Top 20 negative words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I bought my house a year ago, it came with a big, half-dead front lawn.  I decided to do a desert landscape out there instead, & called a number of landscape contractors I found here on Yelp & also at the Home & Garden Show at the fairgrounds.\\n\\nI soon learned that this is an unprofessional industry overall.  I had several company representatives never return my call to set up an appointment to come over & do a survey.  I had others schedule an appointment & not show up.  I had others come & do the initial survey but never get back in touch with me with a price.  One guy I called said he would look at my yard on Google Earth & email me a survey.  (He never did, & I guess a $3500 job didn\\'t make it worth his while to come over for a few minutes!)  I had a couple of others who got back in touch but with only a materials list.  I had asked for a sketch of what they would DO with my yard for all this money!  After probably more than a dozen tries, I finally had 3 legitimate, complete surveys in hand to decide between, & the price range only varied about $500.\\n\\nOne of my surveys was from Tom Baird at Creative Environments.  After the survey, he emailed me a materials list & a CAD drawing that gave me a good idea what my new yard would look like.  I liked it, so I went with them.  Next, though, my job got handed off to another company rep, David Inness.  David was likable, but the job ended up changing considerably from the agreement I had with Mr. Baird.  For instance, I was promised that a BobCat skip loader would be used to dredge down 4\" to remove the old grass before the new materials were applied.  That & other promises were not fulfilled.\\n\\nOn installation day, three workers (& no \"Foreman Chico\" whom I was promised would be here) came out to do the work.  They didn\\'t remove the grass at all, but rather put down some black plastic material over about HALF (!) of the yard.  I complained, & one of the workers called Chico on his cell phone & handed it to me.  Chico explained to me that they weren\\'t going to dig down, or put down any more plastic, unless I wanted to go to the hardware store & buy some more myself!  I tried to call David Inness, but he was unreachable.\\n\\nThe materials included a big pile of dirt & a big pile of gravel to be used for the job.  The workers hand shoveled the dirt to form the contoured mounds as planned, but then only applied a very thin coating of the gravel on top, leaving most of the big pile on the driveway, AND LEFT!  That was it, they told me, & that I could use the rest of the tons of gravel that I had bought in planters around the place, or if I wanted, they would have it picked up in a couple days!\\n\\nAfter trying to call for a day or two, I finally reached David Inness, who got the workers to come back over & finish applying the gravel, which they did, but unevenly.  The gravel depth ended up being zero to 3\" deep.  I moved it around some myself to even it up, but I shouldn\\'t have had to, & since I didn\\'t get the dig-down preparation, the gravel was already skating out onto the surrounding driveway & sidewalk.  David had the workers now dig down a little around the edges, so that the top of the gravel would be down about an inch to stay put better.\\n\\nThis was the worst contractor experience I have ever had in my life.  The only thing saving it from a 1-Star rating is that the nice plan for the layout of the dirt & plants was followed, & the yard does look good now.  Of course I have a problem with grass coming up through the gravel, since it wasn\\'t scooped out as promised.  (I attempted to kill it ahead of time with weed & grass killer as directed, but all my estimators told me you really need to do both.)\\n\\nIf I had it to do over, I would have gone with Michael at Breese Landscaping instead.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches( dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def generate_batches(dataset, batch_size, shuffle=True,drop_last=True, device=\"cuda\"):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "    shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        print(\"-----\")\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "    yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "0\n",
      "{'x_data': tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'y_target': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
